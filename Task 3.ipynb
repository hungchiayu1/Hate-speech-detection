{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec864e37",
   "metadata": {},
   "source": [
    "# Models tried : Logistic Regression, Naive Bayes, XGBoost and combination of 3\n",
    "\n",
    "# Total number of model is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606914a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12476b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe26902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_post(row):\n",
    "    ## Stem all words in the post\n",
    "    words = row.split()  \n",
    "    result = \"\"\n",
    "    for word in words:\n",
    "        result+=ps.stem(word)+' '\n",
    "    return result[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc0ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['stemmed_post'] = df['post'].apply(stem_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5609da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe2d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243cdd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom evaluation metric for xgb\n",
    "def f1_eval_mac(predt, d):\n",
    "    y = d.get_label()\n",
    "\n",
    "    predt_binary = (predt>0.5).astype(int)\n",
    "    \n",
    "    return \"F1_score\", f1_score(y_true=y, y_pred=predt_binary,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b63d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train xgb given the train_df,test_df and hyperparameters\n",
    "def xgbtrain(train_df,test_df,param,count_vec):\n",
    "   \n",
    " \n",
    "    xtrain = count_vec.transform(train_df['stemmed_post']) \n",
    "    xtest = count_vec.transform(test_df['stemmed_post'])\n",
    "\n",
    "    dtrain = xgb.DMatrix(xtrain,label=train_df['label'].to_numpy()) \n",
    "    dtest = xgb.DMatrix(xtest,label=test_df['label'].to_numpy())\n",
    "\n",
    "    evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "    param['eval_metric']= ['auc']\n",
    "    num_round = param[\"n_round\"]\n",
    "    bst = xgb.train(param, dtrain, num_round, evallist,custom_metric=f1_eval_mac)\n",
    "    \n",
    "    ## Score is the f1 score of this model evaluated on the given test set\n",
    "    score = f1_score(np.round(bst.predict(dtest)),test_df['label'],average='macro')\n",
    "    \n",
    "    \n",
    "    return bst,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create countVectorizer for creating TF features\n",
    "\n",
    "count_vec = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    strip_accents=\"ascii\", lowercase=True,\n",
    "    token_pattern=None,ngram_range=(1,2)) ## Create TF features instead of TF-IDF features with unigrams and bigram\n",
    "count_vec.fit(df.stemmed_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8485c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  K fold cross validation on xgb for hyperparameter tuning\n",
    "def k_fold_xgb(df,param,n_fold=10):\n",
    "    skf = StratifiedKFold(n_splits=n_fold, random_state=1, shuffle=True)\n",
    "    f1_score_list = []\n",
    "    Y = df['label']\n",
    "    bst_list = []\n",
    "    for train_index,test_index in skf.split(df,Y):\n",
    "        train_df,test_df = df.loc[train_index],df.loc[test_index] ## Get the train test df on current split\n",
    "        bst,score = xgbtrain(train_df,test_df,param,count_vec)## train it on xgboost\n",
    "        bst_list.append(bst)\n",
    "        f1_score_list.append(score) \n",
    "        \n",
    "    return bst_list,np.average(f1_score_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d274f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default parameters\n",
    "param = {'max_depth': 12, 'eta': 0.1, 'objective': 'binary:logistic',\n",
    "        \"subsample\":0.8,\"colsample_bytree\":0.8,'scale_pos_weight':1.62,'alpha':0.2,\n",
    "        \"min_child_weight\":1,\"n_round\":400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63061422",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test training  \n",
    "\n",
    "## Dont have to run this cell\n",
    "bst_list,avg_f1_score = k_fold_xgb(df,param,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run xgboost prediction with a trained xgb model and a pd dataframe containing the stemmed post\n",
    "def xgb_predict(df,model,count_vec):\n",
    "    xtrain = count_vec.transform(df['stemmed_post'])\n",
    "    return model.predict(xgb.DMatrix(xtrain))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    " ## \n",
    "def batch_xgb_predict(bst_list,df):\n",
    "    temp = []\n",
    "    for bst in bst_list:\n",
    "        res = xgb_predict(df_test,bst,count_vec)\n",
    "        temp.append(res)\n",
    "    final = np.average(np.array(temp),axis=0)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic regression does not need hyper parameter tuning\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "def logistic_regression(train_df):\n",
    "    \"\"\" Train logistic regression model given the stemmed twitter post.\n",
    "        Return: trained model and fitted CountVectorizer \n",
    "    \"\"\"\n",
    "    count_vec = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    strip_accents=\"ascii\", lowercase=True,\n",
    "    token_pattern=None,ngram_range=(1,2))\n",
    "    count_vec.fit(train_df.stemmed_post)\n",
    "    model = BaggingClassifier(base_estimator=linear_model.LogisticRegression(max_iter=200),\n",
    "            n_estimators=10, random_state=0)\n",
    "# fit the model on training data reviews and sentiment\n",
    "    xtrain = count_vec.transform(train_df['stemmed_post'])\n",
    "\n",
    "    model.fit(xtrain, train_df.label)\n",
    "    \n",
    "    return model,count_vec\n",
    "\n",
    "## Naive bayes method \n",
    "def naive_b(train_df):\n",
    "    \n",
    "    \"\"\" Train naive bayes model given the stemmed twitter post.\n",
    "        Return: trained model and fitted CountVectorizer \"\"\"\n",
    "    count_vec = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    strip_accents=\"ascii\", lowercase=True,\n",
    "    token_pattern=None)\n",
    "    count_vec.fit(train_df.stemmed_post)\n",
    "    clf = BaggingClassifier(base_estimator=naive_bayes.MultinomialNB(),\n",
    "            n_estimators=10, random_state=0)\n",
    "    x_train = count_vec.transform(train_df['stemmed_post'])\n",
    "   \n",
    "    clf.fit(x_train,train_df['label'])\n",
    "\n",
    "    return clf,count_vec\n",
    "\n",
    "def model_predict(df,model,count_vec):\n",
    "    ## Predict given a df of stemmed post, model and a CountVectorizer that the model is trained on\n",
    "    xtrain = count_vec.transform(df['stemmed_post'])\n",
    "    return model.predict_proba(xtrain)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "## Average the probability predict for xgboost, logreg and naive bayes\n",
    "\n",
    "def combine_pred(pred_list,test_df):\n",
    "   \n",
    "    return  f1_score(np.round(np.sum(pred_list,axis=0)/3),test_df['label'],average='macro')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train logreg model on the dataset\n",
    "\n",
    "logreg,logreg_count_vec = logistic_regression(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464aa2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train naive bayes model on the dataset\n",
    "naive,naive_count_vec = naive_b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff94f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions for logreg model\n",
    "## Used for voting in the ensembled model later \n",
    "pred1 = model_predict(df_test,logreg,logreg_count_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10710882",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions for naive bayes model\n",
    "\n",
    "## Used for voting in the ensembled model later \n",
    "pred2 = model_predict(df_test,naive,naive_count_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478d580",
   "metadata": {},
   "source": [
    "# Using wandb for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6776c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb is an experiment tracking tool for machine learning\n",
    "\n",
    "## Need to pip install wandb for hyperparameter tuning \n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"grid\", # try grid or random\n",
    "    \"metric\": {\n",
    "      \"name\": \"accuracy\",\n",
    "      \"goal\": \"maximize\"   \n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"max_depth\": {\n",
    "            \"values\": [12,15]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1]\n",
    "        },\n",
    "        \"subsample\": {\n",
    "            \"values\": [0.8]\n",
    "        },\n",
    "        \"colsample_bytree\": {\n",
    "            \"values\": [0.8]\n",
    "        },\n",
    "        \"alpha\": {\n",
    "            \"values\": [1, 0.5, 0.1,0]\n",
    "        },\n",
    "        \"gamma\": {\n",
    "            \"values\": [1, 0.1,0.3,0]\n",
    "        },\n",
    "        \"n_estimators\": {\n",
    "            \"values\": [400]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0,0.1,0.05,1]\n",
    "        }\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5143dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"XGBoost-sweeps12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ecad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_defaults = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 1,\n",
    "    \"seed\": 117,\n",
    "    \"test_size\": 0.33,\n",
    "      }\n",
    "\n",
    "    wandb.init(config=config_defaults)  # defaults are over-ridden during the sweep\n",
    "    config = wandb.config\n",
    "\n",
    "    ## set xgb param from sweep config\n",
    "    param = {\"max_depth\":config.max_depth,\"eta\":config.learning_rate,\n",
    "             'objective': 'binary:logistic',\n",
    "            \"subsample\":config.subsample,\"colsample_bytree\":config.colsample_bytree,\n",
    "             \"alpha\":config.alpha,\"gamma\":config.gamma,'scale_pos_weight':1.62,\"n_round\":config.n_estimators,\"lambda\":config.lambd\n",
    "            }\n",
    "    \n",
    "    bst_list,avg_f1_score = k_fold_xgb(df,param,10)\n",
    "   \n",
    "   \n",
    "    print(f\"f1_score: {int(avg_f1_score * 100.)}%\")\n",
    "    wandb.log({\"f1_score\": avg_f1_score})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e683257",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best parameters found using grid search\n",
    "param = {'max_depth': 12, 'eta': 0.1, 'objective': 'binary:logistic',\n",
    "        \"subsample\":0.8,\"colsample_bytree\":0.8,'scale_pos_weight':1.62,'alpha':0.1,\"gamma\":0.3,\"lambda\":1,\n",
    "        \"min_child_weight\":1,\"n_round\":450}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd92cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bst_list,avg_f1_score = k_fold_xgb(df,param,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## label prediction for xgb model\n",
    "pred3 = batch_xgb_predict(bst_list,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = np.array([pred1,pred2,pred3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba044e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average the predictions\n",
    "final = np.round(np.average(pred_list,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test['label'] = final.astype(int)\n",
    "df_final = df_test.drop([\"post\",\"stemmed_post\"],axis=1)\n",
    "df_final = df_final.set_index('id')\n",
    "df_final.to_csv(\"test_submit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
